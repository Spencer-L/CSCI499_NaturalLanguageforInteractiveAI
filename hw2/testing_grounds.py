import argparse
import os
import tqdm
import torch
from sklearn.metrics import accuracy_score
from torch.utils.data import TensorDataset
import numpy as np

from eval_utils import downstream_validation
import utils
import data_utils

from utils import(
    create_train_val_splits
)

def main(args):
    """
       return:
           - train_loader: torch.utils.data.Dataloader
           - val_loader: torch.utils.data.Dataloader
       """

    # read in training data from books dataset
    sentences = data_utils.process_book_dir(args.data_dir)

    # build one hot maps for input and output
    (
        vocab_to_index,
        index_to_vocab,
        suggested_padding_len,
    ) = data_utils.build_tokenizer_table(sentences, vocab_size=args.vocab_size)

    # create encoded input and output numpy matrices for the entire dataset and then put them into tensors
    encoded_sentences, lens = data_utils.encode_data(
        sentences,
        vocab_to_index,
        suggested_padding_len,
    )

    # ================== TODO: CODE HERE ================== #
    # Task: Given the tokenized and encoded text, you need to
    # create inputs to the LM model you want to train.
    # E.g., could be target word in -> context out or
    # context in -> target word out.
    # You can build up that input/output table across all
    # encoded sentences in the dataset!
    # Then, split the data into train set and validation set
    # (you can use utils functions) and create respective
    # dataloaders.
    # ===================================================== #

    # create train/val splits
    # train_words, val_words = create_train_val_splits(encoded_sentences)

    # make contexts
    # def transform_sentence(sentence, pbar):
    #     pbar.update(1)
    #     context_target = []
    #     for i in range(2, len(sentence) - 2):
    #         context = [sentence[i + 2], sentence[i + 1],
    #                    sentence[i - 1], sentence[i - 2]]
    #         target = sentence[i]
    #         context_target.append((context, target))
    #         if sentence[i + 2] == vocab_to_index['<end>']:
    #             break
    #     return np.array(context_target)
    #
    # with tqdm.tqdm(total=len(encoded_sentences)) as pbar:
    #     context_target = np.vectorize(transform_sentence)(encoded_sentences, pbar)
    #     print(context_target.shape)

    X = []
    Y = []
    encoded_sentences = encoded_sentences[:int(0.25 * len(encoded_sentences))]
    for sentence in tqdm.tqdm(encoded_sentences, desc="Sentences"):
        for i in range(2, len(sentence) - 2):
            X.append(
                [sentence[i + 2], sentence[i + 1],
                 sentence[i - 1], sentence[i - 2]]
                     )
            Y.append(sentence[i])
            if sentence[i + 2] == vocab_to_index['<end>']:
                break

    print("finished context target transformation")

    X = np.array(X)
    Y = np.array(Y)

    cut_off = int(len(X)*0.9)
    # np.random.shuffle(context_target)
    # print("shuffling completed")
    x_train, y_train = X[:cut_off], Y[:cut_off]
    x_val, y_val = X[cut_off + 1:], Y[cut_off+1:]
    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)


    # make data loaders
    train_dataset = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(y_train))
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_dataset = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(y_val))
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)

    return train_loader, val_loader

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, help="where the book dataset is stored")
    parser.add_argument(
        "--vocab_size", type=int, default=3000, help="size of vocabulary"
    )

    # ======================= NOTE ======================== #
    # If you adjust the vocab_size down below 3000, there
    # may be analogies in the downstream evaluation that have
    # words that are not in your vocabulary, resulting in
    # automatic (currently) zero score for an ABCD where one
    # of A, B, C, or D is not in the vocab. A visible warning
    # will be generated by the evaluation loop for these examples.
    # ===================================================== #

    # ================== TODO: CODE HERE ================== #
    # Task (optional): Add any additional command line
    # parameters you may need here
    # ===================================================== #

    args = parser.parse_args()
    main(args)