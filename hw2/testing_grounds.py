import argparse
import os
import tqdm
import torch
from sklearn.metrics import accuracy_score
from torch.utils.data import TensorDataset
import numpy as np

from eval_utils import downstream_validation
import utils
import data_utils

from utils import(
    create_train_val_splits
)

def main(args):
    """
       return:
           - train_loader: torch.utils.data.Dataloader
           - val_loader: torch.utils.data.Dataloader
       """

    # read in training data from books dataset
    sentences = data_utils.process_book_dir(args.data_dir)

    # build one hot maps for input and output
    (
        vocab_to_index,
        index_to_vocab,
        suggested_padding_len,
    ) = data_utils.build_tokenizer_table(sentences, vocab_size=args.vocab_size)

    # create encoded input and output numpy matrices for the entire dataset and then put them into tensors
    encoded_sentences, lens = data_utils.encode_data(
        sentences,
        vocab_to_index,
        suggested_padding_len,
    )

    # ================== TODO: CODE HERE ================== #
    # Task: Given the tokenized and encoded text, you need to
    # create inputs to the LM model you want to train.
    # E.g., could be target word in -> context out or
    # context in -> target word out.
    # You can build up that input/output table across all
    # encoded sentences in the dataset!
    # Then, split the data into train set and validation set
    # (you can use utils functions) and create respective
    # dataloaders.
    # ===================================================== #

    # create train/val splits
    train_words, val_words = create_train_val_splits(encoded_sentences)

    # make contexts
    train_context = []
    for i in range(2, len(train_words) - 2):
        context = [train_words[i + 2], train_words[i + 1],
                   train_words[i - 1], train_words[i - 2]]
        target = train_words[i]
        train_context.append((context, target))

    val_context = []
    for i in range(2, len(val_words) - 2):
        context = [val_words[i + 2], val_words[i + 1],
                   val_words[i - 1], val_words[i - 2]]
        target = val_words[i]
        val_context.append((context, target))

    # make data loaders
    train_dataset = TensorDataset(torch.from_numpy(train_context), torch.from_numpy(lens))
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_dataset = TensorDataset(torch.from_numpy(val_context), torch.from_numpy(lens))
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)

    return train_loader, val_loader

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data_dir", type=str, help="where the book dataset is stored")
    parser.add_argument(
        "--vocab_size", type=int, default=3000, help="size of vocabulary"
    )

    # ======================= NOTE ======================== #
    # If you adjust the vocab_size down below 3000, there
    # may be analogies in the downstream evaluation that have
    # words that are not in your vocabulary, resulting in
    # automatic (currently) zero score for an ABCD where one
    # of A, B, C, or D is not in the vocab. A visible warning
    # will be generated by the evaluation loop for these examples.
    # ===================================================== #

    # ================== TODO: CODE HERE ================== #
    # Task (optional): Add any additional command line
    # parameters you may need here
    # ===================================================== #

    args = parser.parse_args()
    main(args)